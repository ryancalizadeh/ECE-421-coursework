{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SVM on Iris Dataset</h1>\n",
    "\n",
    "For this problem, we will empirically compare the performance of a binary linear classifier to a SVM. Because the purpose of this problem is to compare the performance of the two classifiers, you can use their im- plementation in sklearn. However, note that (a) you should understand how to implement these classifiers from scratch and (b) you may be evaluated on this later in the course. Consider the iris dataset included in sklearn. For all questions of this problem (except the last question), we will only consider the first 100 entries of the dataset. With the help\n",
    "of train test split from sklearn.model selection, split the dataset into a training set and a test set. For now, you can do so by setting the argument test size to 0.8 in train test split. To ensure results below are comparable and reproducible, set the random state argument of your train test split calls to 0: this will control the shuffling applied to the data before applying the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "from xml.sax import xmlreader\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a binary linear classifier on the first two dimensions (sepal length and width) of the iris dataset and plot its decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_iris(return_X_y=True)\n",
    "X_100 = X[:100, :2]\n",
    "Y_100 = Y[:100]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_100, Y_100, test_size=0.8, random_state=0)\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1])\n",
    "\n",
    "def predict(x, clf):\n",
    "    m = clf.coef_[0][0] / -clf.coef_[0][1]\n",
    "    b = clf.intercept_ / -clf.coef_[0][1]\n",
    "    return  m*x + b\n",
    "\n",
    "t = np.arange(4.5, 8.5)\n",
    "line = predict(t, clf)\n",
    "plt.plot(t, line)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Report the accuracy of your binary linear classifier on both the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = clf.predict(X_train)\n",
    "train_accuracy = np.sum(z_train == y_train) / len(y_train)\n",
    "z_test = clf.predict(X_test)\n",
    "test_accuracy = np.sum(z_test == y_test) / len(y_test)\n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy) # 1.0\n",
    "print(\"Test Accuracy: \", test_accuracy) # 0.9875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a linear SVM classifier on the first two dimensions (sepal length and width). Plot the decision boundary of the classifier and its margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svm = svm.SVC(kernel=\"linear\", C=100).fit(X_train, y_train)\n",
    "\n",
    "t = np.arange(4.5, 6.5, 0.1)\n",
    "svm_line = predict(t, my_svm)\n",
    "line_data = np.array([t, svm_line])\n",
    "\n",
    "margin1 = np.array(line_data.T + (my_svm.coef_) / np.sum(np.square(my_svm.coef_)))\n",
    "margin2 = np.array(line_data.T - (my_svm.coef_) / np.sum(np.square(my_svm.coef_)))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1])\n",
    "plt.plot(t, line_data[1])\n",
    "plt.plot(margin1[:, 0], margin1[:, 1], \"blue\")\n",
    "plt.plot(margin2[:, 0], margin2[:, 1], \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Report the accuracy of your linear SVM classifier on both the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train = my_svm.predict(X_train)\n",
    "train_accuracy = np.sum(z_train == y_train) / len(y_train)\n",
    "z_test = my_svm.predict(X_test)\n",
    "test_accuracy = np.sum(z_test == y_test) / len(y_test)\n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy) # 1.0\n",
    "print(\"Test Accuracy: \", test_accuracy) # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the value of the margin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Boundary: \", 2 * np.linalg.norm(1 / my_svm.coef_)) # 0.8486015315113424"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Split the iris dataset again in a training and test set, this time setting test size to 0.4 when calling train test split. Train the SVM classifier again. Does the decision boundary change? How about the test accuracy? Please justify why (hint: think about the support vectors), and illustrate your argument with a new plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_100, Y_100, test_size=0.8, random_state=0)\n",
    "my_svm = svm.SVC(kernel=\"linear\", C=100).fit(X_train, y_train)\n",
    "print(\"Decision Boundary: \", 2 * np.linalg.norm(1 / my_svm.coef_)) # 0.8486015315113424\n",
    "\n",
    "z_train = my_svm.predict(X_train)\n",
    "train_accuracy = np.sum(z_train == y_train) / len(y_train)\n",
    "z_test = my_svm.predict(X_test)\n",
    "test_accuracy = np.sum(z_test == y_test) / len(y_test)\n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy) # 1.0\n",
    "print(\"Test Accuracy: \", test_accuracy) # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Do the binary linear classifier and SVM have the same decision boundaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Decision Boundary: \", clf.coef_, clf.intercept_) # [[ 1.80226162 -1.24492959]] [-5.9685275]\n",
    "print(\"SVM Decision Boundary: \", my_svm.coef_, my_svm.intercept_) # [[ 3.33266363 -3.33342658]] [-7.66277845]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now consider all 150 entries in the iris dataset, and retrain the SVM. You should find that the data points are not linearly separable. How can you deal with it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a polynnomial kernel as follows:\n",
    "my_svm = svm.SVC(kernel=\"poly\", C=100).fit(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
